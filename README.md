# rag_fastapi_llm
Client   ↓ FastAPI endpoint (/ai/generate)   ↓ Retrieve relevant documents (Vector DB)   ↓ Augment the prompt with retrieved context   ↓ LLM generates grounded response
